{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "502607c1-3bc3-4dc0-835f-952d745d4fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/hpc/home/mk423/.local/lib/python3.7/site-packages/lpne/pipelines/__init__.py:14: UserWarning: Could not load lpne/pipelines/default_params.yaml!\n",
      "  warnings.warn(\"Could not load lpne/pipelines/default_params.yaml!\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2032734/4000186618.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepm_data_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mepm_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moft_data_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from lpne.models import DcsfaNmf\n",
    "from lpne.plotting import circle_plot\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "\n",
    "umc_data_tools_path = \"/hpc/home/mk423/Anxiety/Universal-Mouse-Code/\"\n",
    "sys.path.append(umc_data_tools_path)\n",
    "import umc_data_tools as umc_dt\n",
    "\n",
    "N_COMPONENTS=30\n",
    "\n",
    "fold = 1#int(os.environ['SLURM_ARRAY_TASK_ID'])\n",
    "\n",
    "flx_data_path = \"/work/mk423/Anxiety/flx_kf_dict_fold_{}.pkl\".format(fold)\n",
    "epm_data_path = \"/work/mk423/Anxiety/epm_kf_dict_fold_{}.pkl\".format(fold)\n",
    "oft_data_path = \"/work/mk423/Anxiety/oft_kf_dict_fold_{}.pkl\".format(fold)\n",
    "anx_info_dict = \"/work/mk423/Anxiety/Anx_Info_Dict.pkl\"\n",
    "\n",
    "saved_model_path = \"/work/mk423/Anxiety/kfold_models/\"\n",
    "saved_model_name = \"bootstrap_321_net_{}_model.pt\".format(fold)\n",
    "\n",
    "results_path = \"/hpc/home/mk423/Anxiety/FullDataWork/Validations/\"\n",
    "results_file = results_path + \"bootstrap_321_net_{}_results.pkl\".format(fold)\n",
    "\n",
    "projection_save_path = \"/hpc/home/mk423/Anxiety/FullDataWork/Projections/\"\n",
    "plots_path = \"/hpc/home/mk423/Anxiety/FullDataWork/Figures/\"\n",
    "\n",
    "\n",
    "def reshapeData(X_psd,X_coh,n_rois,n_freqs,pow_features,coh_features,areas):\n",
    "    X_3d = np.zeros((n_rois,n_rois,n_freqs))\n",
    "    \n",
    "    for i in range(n_rois):\n",
    "        X_3d[i,i,:] = X_psd[i*n_freqs:(i+1)*n_freqs]\n",
    "        \n",
    "    \n",
    "    split_coh_features = np.array([feature.split(' ')[0] for feature in coh_features])\n",
    "    #print(split_coh_features)\n",
    "    unique_coh_features = np.unique(split_coh_features)\n",
    "    for i in range(n_rois):\n",
    "        for j in range(n_rois):\n",
    "            if i != j:\n",
    "                area_1 = areas[i]\n",
    "                area_2 = areas[j]\n",
    "                temp_feature = area_1 + \"-\" + area_2\n",
    "                temp_feature_2 = area_2 + \"-\" + area_1\n",
    "                if temp_feature in unique_coh_features:\n",
    "                    feature_mask = np.where(split_coh_features==temp_feature,True,False)\n",
    "                    X_3d[i,j,:] = X_coh[feature_mask==1]\n",
    "                    X_3d[j,i,:] = X_coh[feature_mask==1]\n",
    "\n",
    "                elif temp_feature_2 in unique_coh_features:\n",
    "                    feature_mask = np.where(split_coh_features==temp_feature_2,1,0)\n",
    "                    X_3d[i,j,:] = X_coh[feature_mask==1]\n",
    "                    X_3d[j,i,:] = X_coh[feature_mask==1]\n",
    "\n",
    "                else:\n",
    "                    print(\"temp_feature: {} not found\".format(temp_feature))\n",
    "\n",
    "    return X_3d\n",
    "\n",
    "with open(flx_data_path,\"rb\") as f:\n",
    "    flx_dict = pickle.load(f)\n",
    "    \n",
    "with open(epm_data_path,\"rb\") as f:\n",
    "    epm_dict = pickle.load(f)\n",
    "    \n",
    "with open(oft_data_path,\"rb\") as f:\n",
    "    oft_dict = pickle.load(f)\n",
    "    \n",
    "with open(anx_info_dict,\"rb\") as f:\n",
    "    anxInfo = pickle.load(f)\n",
    "\n",
    "info_dict = anxInfo\n",
    "feature_groups = [(0,len(info_dict[\"powerFeatures\"])),\n",
    "                   (len(info_dict[\"powerFeatures\"]),len(info_dict[\"powerFeatures\"])+len(info_dict[\"cohFeatures\"])),\n",
    "                   (len(info_dict[\"powerFeatures\"])+len(info_dict[\"cohFeatures\"]),\n",
    "                    len(info_dict[\"powerFeatures\"])+len(info_dict[\"cohFeatures\"])+len(info_dict[\"gcFeatures\"]))]\n",
    "                   \n",
    "mt_X_train = np.vstack([flx_dict[\"X_train\"],epm_dict[\"X_train\"],oft_dict[\"X_train\"]])\n",
    "mt_y_train = np.hstack([flx_dict[\"y_train\"],epm_dict[\"y_train\"],oft_dict[\"y_train\"]]).reshape(-1,1)\n",
    "mt_y_train_3_net = np.hstack([mt_y_train,mt_y_train,mt_y_train])\n",
    "mt_y_mouse_train = np.hstack([flx_dict[\"y_mouse_train\"],epm_dict[\"y_mouse_train\"],oft_dict[\"y_mouse_train\"]])\n",
    "\n",
    "mt_y_exp_train = np.hstack([np.ones(flx_dict[\"X_train\"].shape[0])*0,\n",
    "                           np.ones(epm_dict[\"X_train\"].shape[0]),\n",
    "                           np.ones(oft_dict[\"X_train\"].shape[0])*2])\n",
    "intercept_mask = OneHotEncoder().fit_transform(mt_y_mouse_train.reshape(-1,1)).todense()\n",
    "sample_groups = OrdinalEncoder().fit_transform(mt_y_mouse_train.reshape(-1,1))\n",
    "\n",
    "mt_X_val = np.vstack([flx_dict[\"X_val\"],epm_dict[\"X_val\"],oft_dict[\"X_val\"]])\n",
    "mt_y_val = np.hstack([flx_dict[\"y_val\"],epm_dict[\"y_val\"],oft_dict[\"y_val\"]]).reshape(-1,1)\n",
    "mt_y_val_3_net = np.hstack([mt_y_val,mt_y_val,mt_y_val])\n",
    "mt_y_mouse_val = np.hstack([flx_dict[\"y_mouse_val\"],epm_dict[\"y_mouse_val\"],oft_dict[\"y_mouse_val\"]])\n",
    "\n",
    "model = torch.load(saved_model_path + saved_model_name)\n",
    "model.eval()\n",
    "#Multitask Performance\n",
    "mt_train_auc = model.score(mt_X_train,mt_y_train)\n",
    "mt_val_auc = model.score(mt_X_val,mt_y_val)\n",
    "\n",
    "#FLX Performance\n",
    "\n",
    "flx_y_train = np.hstack([flx_dict[\"y_train\"].reshape(-1,1),flx_dict[\"y_train\"].reshape(-1,1)])\n",
    "flx_y_val = flx_dict[\"y_val\"].reshape(-1,1)\n",
    "\n",
    "flx_train_auc = model.score(flx_dict[\"X_train\"],flx_y_train,\n",
    "                           flx_dict['y_mouse_train'],return_dict=True)\n",
    "flx_val_auc = model.score(flx_dict[\"X_val\"],flx_y_val,\n",
    "                          flx_dict[\"y_mouse_val\"],return_dict=True)\n",
    "\n",
    "#EPM Performance\n",
    "epm_y_train = np.hstack([epm_dict[\"y_train\"].reshape(-1,1),epm_dict[\"y_train\"].reshape(-1,1)])\n",
    "epm_y_val = epm_dict[\"y_val\"].reshape(-1,1)\n",
    "\n",
    "epm_train_auc = model.score(epm_dict[\"X_train\"],epm_y_train,\n",
    "                           epm_dict[\"y_mouse_train\"],return_dict=True)\n",
    "epm_val_auc = model.score(epm_dict[\"X_val\"],epm_y_val,\n",
    "                          epm_dict[\"y_mouse_val\"],return_dict=True)\n",
    "\n",
    "#OFT Performance\n",
    "oft_y_train = np.hstack([oft_dict[\"y_train\"].reshape(-1,1),oft_dict[\"y_train\"].reshape(-1,1)])\n",
    "oft_y_val = oft_dict[\"y_val\"].reshape(-1,1)\n",
    "\n",
    "oft_train_auc = model.score(oft_dict[\"X_train\"],oft_y_train,\n",
    "                            oft_dict['y_mouse_train'],return_dict=True)\n",
    "oft_val_auc = model.score(oft_dict[\"X_val\"],oft_y_val,\n",
    "                          oft_dict['y_mouse_val'],return_dict=True)\n",
    "\n",
    "print(\"\\nflx train\",flx_train_auc)\n",
    "print(\"\\nflx val\",flx_val_auc)\n",
    "print(\"\\nepm train\",epm_train_auc)\n",
    "print(\"\\nepm val\",epm_val_auc)\n",
    "print(\"\\noft train\",oft_train_auc)\n",
    "print(\"\\noft val\",oft_val_auc)\n",
    "\n",
    "s = model.project(mt_X_val)\n",
    "X_sup_recon = model.get_comp_recon(torch.Tensor(s).to(\"cuda\"),0)\n",
    "X_sup_recon_2 = model.get_comp_recon(torch.Tensor(s).to(\"cuda\"),1)\n",
    "X_sup_recon_3 = model.get_comp_recon(torch.Tensor(s).to(\"cuda\"),2)\n",
    "X_recon = model.reconstruct(mt_X_val)\n",
    "\n",
    "net_1_recon_contribution = np.mean(X_sup_recon/X_recon,axis=0)\n",
    "net_2_recon_contribution = np.mean(X_sup_recon_2/X_recon,axis=0)\n",
    "net_3_recon_contribution = np.mean(X_sup_recon_3/X_recon,axis=0)\n",
    "\n",
    "results_dict = {\n",
    "    \"flx_train_auc\":flx_train_auc,\n",
    "    \"flx_val_auc\":flx_val_auc,\n",
    "    \"epm_train_auc\":epm_train_auc,\n",
    "    \"epm_val_auc\":epm_val_auc,\n",
    "    \"oft_train_auc\":oft_train_auc,\n",
    "    \"oft_val_auc\":oft_val_auc,\n",
    "    \"recon_cont_net_1\":net_1_recon_contribution,\n",
    "    \"recon_cont_net_2\":net_2_recon_contribution,\n",
    "    \"recon_cont_net_3\":net_3_recon_contribution,\n",
    "}\n",
    "\n",
    "with open(results_file,\"wb\") as f:\n",
    "    pickle.dump(results_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da983658-0baa-4103-9d9f-e187c22c7e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
